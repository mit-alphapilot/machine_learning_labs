{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MIT AlphaPilot Machine Learning Labs Gate Detection Lab Imitation Learning Lab Reinforcement Learning Lab","title":"MIT AlphaPilot"},{"location":"#mit-alphapilot","text":"","title":"MIT AlphaPilot"},{"location":"#machine-learning-labs","text":"Gate Detection Lab Imitation Learning Lab Reinforcement Learning Lab","title":"Machine Learning Labs"},{"location":"gate_detection/","text":"Gate Detection Lab Overview This lab is split into two parts. Part 1 will introduce you to the RACECAR Model-X, its software stack, and how it differs from the RACECARs you've been using throughout the semester. Part 2 will then have you implement simple deep learning approach to object detection on the car. Part 1: RACECAR Model-X The RACECAR Model-X (MX) is quite different from the Model-T (MT) cars used throughout the semester. For instance: - NVIDIA Jetson TX2 has been upgraded for the NVIDIA Xavier (Model-T for TX2, Model-X for Xavier) - LiDAR ( Hokuyo or Velodyne ) has been removed - Logitech F710 Joystick has been switched for a wired Xbox One Controller (due to driver compatibility) - ZED Stereoscopic Camera has been replaced with three Logitech C310 webcams - SparkFun IMU has been removed - Chassis has been redesign to allow space for the taller Xavier dev board and the mounting of multiple webcams While the simplification of the onboard sensor suite may initially seem a step back for the platform, this is an intensional design decision. Unlike the MT cars, which prioritized LiDAR and sensor fusion solutions, the MX cars prioritize exclusively deep learning image based solutions. To this end, the removal of non-imaging sensors (LiDAR, IMU, etc.) and the upgrade to the NVIDIA Xavier makes sense. The Xavier is the most powerful embedded GPU board recently released by NVIDIA. Compared to the TX2s, the Xavier has significant preformance improvements accross the board. Image Source: https://www.fastcompression.com/ Furthermore, the addition of three webcams allows you to get images from different viewpoints as inputs to your algorithms. This will be espeically valuable for later NEET labs. Task 1: Boot Up Although familiar, booting up the MX car is slightly different than the MT. Specifically, the Xavier, battery, and WIFI credentials are slightly different. Start by connecting the barrel connector to the battery and pressing the power button once. Do not press the power button multiple times! The battery used on the MX cars has variable voltage. By default they power on at the correct voltage, 12V. Pressing the power button multiple times will cycle through other voltage settings and risk damaging equipments (particularly the router and USB hub that are only rated to run at 12V). The battery powers off automatically when components stop drawing power for a few minutes. Thus, when you are finished using the car, you shutdown the battery by simply removing the barrel connector from the battery, not by pressing the power button again. Do not press the power button multiple times! Once the battery is plugged in, you should see the LEDs on the router and USB hub light up. Additionally, you can now boot the NVIDIA Xavier by pressing the left-most button on the back of the unit. After allowing approximately 1-2 minutes for everything to boot up, you can start to connect to the car. The WIFI SSID for a given car should be RACECAR_MX_# where # is the number of your car (i.e. 0 , 1 , 2 , or 3 ). Your car, Xavier, controller, battery, and router should all be labeled to help you determine your car number. The WIFI password should be the familiar g0_fast! . Once connected to the WIFI, you should be able to SSH to the car with the following command: ssh racecar@192.168.0.20# again where # is the number of your car. Again, the password should be the familiar racecar@mit . Once connected to your car, you should notice a similar file structure to the MT car. One thing to note is the NVIDIA Xaviers exclusively run Ubuntu 18.04. The TX2 you have been using on the MT cars are still using Ubuntu 16.04. While this should not matter for the labs, it is worth noting in case you try to install additional packages. Task 2: Teleop & Launch Files Once connected to your car, we will prepare to teleop the car for the first time. Before doing this, make sure to connect your Traxxas battery to the VESC and give it sufficient time to boot up and connect to the Xavier as a serial device. Run the command: teleop Recall this is an alias defined in ~/.bashrc for the more verbose command: roslaunch racecar teleop.launch What do you see? Does the car drive? Is anything wrong? Screenshot of teleop running on a fresh MX car Likely, you will observe the car teleops as expected, but your terminal is polluted with errors. This is because by default, the MX car is configured with the same software stack as the standard MT RACECAR. Like all things, this has both pros and cons. The pro here is this is identical code to what you have been using all semster (i.e. the code structure, rostopics, etc.). The con here is that the out-of-the-box defaults are not ideal for the MX car. Upon examining the error messages, you will notice they are in response to missing sensors. Specifically, the LiDAR and IMU (the ZED did run by default from teleop ). Modify your launch files to a configuration that makes sense for the MX car. (Hint: This will just involve removing a few lines to not run drivers for missing sensors.) Task 3: Test Cameras Take a look at the simple Python file ~/webcams.py in your home directory. What do you think it does? Does it run? Screenshot of python ~/webcams.py without running ssh with a -X argument Likely, the program does not run since there is a GUI component. Exit your ssh connection with the car and rejoin with the command ssh -X racecar@192.168.0.20# again where # is the number of your car. The -X argument enables X11 forwarding (i.e. allows you to render remote GUI components on your local machine). Try running the script again. Does it run now? Is there delay? Screenshot of python ~/webcams.py while running ssh with a -X argument You can expect to see delay in the frames. This is simply because you are streaming uncompressed images. The vehicle should be receiving these images on time. Are the cameras aligned accross the vertical dimenson? What happens if we remove the frame dimension restrictions in software? Why might this happen? Are the cameras displaying in the right order? Fix the camera ordering in software if needed. Task 4: Write ROS Node Finally, we will write our very own ROS Node for our accessing these pictures within the ROS ecosystem. Using ~/webcams.py as a starting point, modify the code to produce a useful rostopic interface. You will be using this interface for the rest of these NEET Labs. (Hint: Recall how the /zed/ topics were structured. Would something similar work?) Once your ROS Node is complete and tested, put the file in a sensible place within the ~/racecar_ws file hierarchy. Do not add it to your launch script. You will want to selectively run this script in the future. Part 2: Gate Detection In this lab we will be creating a weakly-supervised deep learning system for detecting gates in TensorFlow. This lab is inspired by the methodology outline in a paper presented at CVPR 2015 called Is object localization for free? - Weakly-supervised learning with convolutional neural networks If you find this material interested, we highly encourage you to read the original paper. Background on ML The following Topic Sections will review some fundamental machine learning topics neccessary for this lab. There are no tasks in this section. If you are already familiar and comfortable with these topics, feel free to jump to the Task Sections and reference this information as needed. This background material is adapted from a previous lab offered in section of 6.a01. [Source] Topic 1: What is a Tensor? Tensor is a term that gets thrown around a lot these days in the context of machine learning. For instance, TensorFlow, the machine learning library we will be using for the remainder of the NEET assignments, gets its namesake from this concept. Put simply, a tensor is just a generalization of vector, matrix, etc. A scaler is a 0-dimensional tensor, a vector is a 1-dimensional tensor, a matrix is a 2-dimension tensor, etc. The rank of a tensor is simply how many dimensions the tensor has. Similarly, a scaler is rank 0, a vector is rank 1, a matrix is rank 2, etc. In this lab, the input to our neural network will be color images coming from the webcams in Part 1. Can you guess the rank and shape of the tensor representation? (Hint: the dimensions are ordered as [height, width, channels] ). There are many different color spaces used to represent digital photos. The most common is RGB. If you said, [240, 320, 3] , you are absolutely correct! Keeping this straight is critical but also challenging (especially since some libraries alternate between wanting coordinates in (x,y) and other times in (r,c) . This is further complicated when you group multiple images into dataset. A group of these images is called a batch , has a rank of 4, and ordered dimensions of [batch_size, height, width, channels] . When you start constructing your own networks, you will notice many bugs boil down to mismatching the of the previous layer's output dimensionality to the next layer's input dimensionality. You can always check the dimensionality of a tensor by printing out the shape property in Python. >>> print(batch.shape) (6000,240,320,3) Topic 2: What are Convolutions and their Uses? A convolution is a mathematical operation with its roots in functional analysis. Although this is the same discrete operation found in probability, statistics, signal processing, etc., here you can primarily think of it as an operation that changes the shape of a tensor through a small number of learned weights. Visually, this can be interpreted as the following diagram: The input image (bottom, blue) is undergoing a convolution to produce the output (top, teal). A small patch of learned weights is slid over every location. At each position, the weights are multiplied with the overlapping input image values and then all summed together to make a new single value pixel. At a given location, this is governed by the equation: In the above gif, there are 9 weights ( w_1 , w_2 , w_3 , ... , w_9 ) and 9 input values ( x_1 , x_2 , x_3 , ... , x_9 ). There is also a bias term, b , that you can disregard in this example since it is 0. Althought the specific mathematical details are beyond the scope of this tutorial, a trained convolutional layer is useful because it learns to extract useful localized features from input data. The first convolutions (bottom layer) recognizes basic features like corners or lines. The next layer (middle layer) combine these features into primitive shapes such as eyes and ears. The last layer (top layer) combines these features into complete images such as faces. Layering convolutions is what builds this feature higherarchy and is what makes deep learning deep. Additionally, in our use case we are interested in determining where an object is in an image. One useful feature of convolutions is that they do not act over the entire image, but on small subsections. Even after multiple convolutions, the top left pixel of the output image is only going to give us information about the top left patch of the input image. If we use convolutions for classification we are actually training a whole bunch of classifiers in parallel. Each pixel of the output will tell us whether an object is located in a particular portion of the image. We can then use this heatmap to detect where our object of interest resides. Warmer pixels indicate regions of the picture that strongly impacted the classification of the overal picture. Task 1: TensorFlow Tutorials Before jumping into using TensorFlow to detect gates, we will first complete a useful tutorial on TensorFlow. This is because, while an industry staple, TensorFlow can have a steep learning curve. This learning curve is well worth it due to TensorFlow's unmatched power and featureful ecosystem. Also, recent supplimental libraries, like Keras, have greatly reduced the complexity of common ML tasks. You may complete this tutorial as a team or individually. The remainder of the NEET labs will build on this fundamental TensorFlow library knowledge, so make sure everyone understands the concepts individually if you choose to work as a group. When completing this tutorial you may run it locally on your personal laptop or remotely from your MX car. We suggest you work remotely on your car since TensorFlow is already installed there. If you want to work locally, you may follow the TensorFlow installation instructions here . To best mimic the tutorial and the MX, we recommend you complete the tutorial using Python3 (TensorFlow on the Xavier currently only works with Python3). Additionally, we recommend you complete the tutorial in a Jupyter Notebook , a handy interactive Python workspace served through a web brower. To configure the Jupyter Notebook server on your MX RACECAR, complete the following steps: After running SSH to access your MX car, confirm the necessary libraries are installed/install them with: sudo apt install jupyter-notebook sudo apt install python3-matplotlib Generate a default server configuration file with the following command: jupyter notebook --generate-config Edit the generated default config file, /home/racecar/.jupyter/jupyter_notebook_config.py , for remote access by uncommenting the following variables and modifying their values: c.NotebookApp.allow_remote_access = False c.NotebookApp.ip = '192.168.0.20X' # where X is the number of your car c.NotebookApp.open_browser = False c.NotebookApp.port = 9999 Make sure you edit the IP address to include your car's number when modifying the config file. If you are interested in further configuring the server, including setting a permanent password, check out the documentation here . Navigate to a working directory of your choice, perhaps ~/tf-tutorial and start the server with the command: jupyter-notebook Among the server console output will URL of the form: http://192.168.0.200:9999/?token=55db505149ca8e1efd2293455fd0ad97a3220e1e084c881a While connected to the car's router, copy this URL into a web browser on your local machine. You should see a web page similar to the picture below. Note: In the future, if you want to use Jupyter Notebooks on your car again, you should only have to repeat steps 4 and 5. Task 2: Collecting Data Now that you are familiar with the basics of TensorFlow, it is time to start building your own gate detection solution. As you may have already began to realize, machine learning can only ever be as good as the data you collect. Thus, in this phase, we're going to get some practice collecting data. Luckily, this is a relatively simple machine learning task so you will not need to collect too much data. Additionally, since this solution is weakly supervised, you will only need to have data labeled as \"contains gate\" and \"does not contain gate\" -- no bounding boxes required. For reference, a TA solution was completed with 4800 320x240 pictures from the front camera only: 2000 training images containing gates 2000 training images not containing gates 200 verification images containing gates 200 verification images not containing gates Data was collected at 10 frames/sec, meaning the whole data collect took approximately 8 minutes. You are welcomed and encouraged to modify these quantities, use the side cameras, etc. Just know if you are spending hours collecting data, something is wrong and you should reach out to us on Slack. Example gate image from collected dataset. To collect data, feel free to modify the simple script record.py included in this repository. Materials for the gates will be left in the cage. Contact a TA if you need access. Additionally, you can examine specific image files collected with the handy tool Eye of GNOME (or eog for short). You can use the tool like this: eog [FILENAME] You can also cycle through images in that directory with the arrow keys and delete individual images with the Delete key. Since this is a GUI tool, make sure you are running your ssh session with the -X argument. Task 3: Training a Model We will start by creating a simple gate classifier. This network will be trained on the data you collected and output a binary classification -- 0 for no gate, 1 for gate. To get started, open up and read through the code in Train.ipynb . You will notice much of this code is reminiscent, and in fact adapted from, the tutorial you completed on the MNIST dataset. There are two primary modifications you must make to this file: 1. Modify the helper function load_data() to properly load the data you collected. 2. Construct a neural network within the # build model structure block. Try a handful of Conv2D layers and end with a handful of Dense layers. Mix a few AveragePooling2D and Flatten layers in between. Most important is to experement. If you're stuck, try implementing Lenet (pictured below) and modify it from there. Which is preforming best on your validation data? For reference, you should shoot for 95% or more accuracy on your training set and 80% or more accuracy on your validation set. The final step of this notebook saves your trained model for use in the next task. Task 4: Testing a Model Now that we have a trained model, we will use it on data taken directly from the webcam and produce a heatmap. To do this, use Heatmap.ipynb . If your model from the previous task worked properly, you can expect to: 1. Correctly classify the picture you take with the webcam as gate or no_gate 2. In a photo of a gate, generate a heatmap that highlights the gate or critical features of the gate. (Note: Although portions may be highlighted in pictures without a gate as long as the entire image is correctly classified as no_gate you will disregard these heatmaps.) Once you have your heatmap, implement a way to generate a steering angle from the heatmap. Consider methods that have you find the center of the high activate regions. Prototype it in the Jupyter Notebook before we integrate it with ROS in the next Task. Task 5: Integrating with MX This step is, unfortunantely, slightly more complicated than ideal due to compatibility issues. Specifically, the Xavier will only run TensorFlow in Python3 while ROS will only run in Python2. As a result, we need to circumvent this by having a network connection between a session of Python3 and Python2. Although the exact architecture is up to you, we recommend having the Python3 as a server responsible for producing steering angles and Python2 as a client responsible for turning a steering angle into a ROS message. With this method, you will copy a bulk of your code from the Jupyter Notebook into your server script (ideally a .py file, not another .ipynb Jupyter Notebook). With this architecture, the server will include code that connects to the webcam, fetchs a picture, runs it through TensorFlow, produces a heatmap, analyzes the heatmap to produce a steering angle, and, when queried by a client, responds with the current steering angle. To handle the networking, we recommend using ZMQ. You can checkout some documentation here . We've also included a sample server and client program in this repo, originating from the ZMQ documentation. hwserver.py hwclient.py Additionally, this documentation shows how to easily define two functions send_array() and recv_array() transfer a np.array between two programs. Note: ZMQ uses a REQ/REP (request/respond) Lockstep Protocol to send messages. For using this library, just make sure each program alternates between sending/receving messages. Otherwise, if you try to send or receive two messages in a row, you'll get an error about the context not being valid.","title":"Gate Detection Lab"},{"location":"gate_detection/#gate-detection-lab","text":"","title":"Gate Detection Lab"},{"location":"gate_detection/#overview","text":"This lab is split into two parts. Part 1 will introduce you to the RACECAR Model-X, its software stack, and how it differs from the RACECARs you've been using throughout the semester. Part 2 will then have you implement simple deep learning approach to object detection on the car.","title":"Overview"},{"location":"gate_detection/#part-1-racecar-model-x","text":"The RACECAR Model-X (MX) is quite different from the Model-T (MT) cars used throughout the semester. For instance: - NVIDIA Jetson TX2 has been upgraded for the NVIDIA Xavier (Model-T for TX2, Model-X for Xavier) - LiDAR ( Hokuyo or Velodyne ) has been removed - Logitech F710 Joystick has been switched for a wired Xbox One Controller (due to driver compatibility) - ZED Stereoscopic Camera has been replaced with three Logitech C310 webcams - SparkFun IMU has been removed - Chassis has been redesign to allow space for the taller Xavier dev board and the mounting of multiple webcams While the simplification of the onboard sensor suite may initially seem a step back for the platform, this is an intensional design decision. Unlike the MT cars, which prioritized LiDAR and sensor fusion solutions, the MX cars prioritize exclusively deep learning image based solutions. To this end, the removal of non-imaging sensors (LiDAR, IMU, etc.) and the upgrade to the NVIDIA Xavier makes sense. The Xavier is the most powerful embedded GPU board recently released by NVIDIA. Compared to the TX2s, the Xavier has significant preformance improvements accross the board. Image Source: https://www.fastcompression.com/ Furthermore, the addition of three webcams allows you to get images from different viewpoints as inputs to your algorithms. This will be espeically valuable for later NEET labs.","title":"Part 1: RACECAR Model-X"},{"location":"gate_detection/#task-1-boot-up","text":"Although familiar, booting up the MX car is slightly different than the MT. Specifically, the Xavier, battery, and WIFI credentials are slightly different. Start by connecting the barrel connector to the battery and pressing the power button once. Do not press the power button multiple times! The battery used on the MX cars has variable voltage. By default they power on at the correct voltage, 12V. Pressing the power button multiple times will cycle through other voltage settings and risk damaging equipments (particularly the router and USB hub that are only rated to run at 12V). The battery powers off automatically when components stop drawing power for a few minutes. Thus, when you are finished using the car, you shutdown the battery by simply removing the barrel connector from the battery, not by pressing the power button again. Do not press the power button multiple times! Once the battery is plugged in, you should see the LEDs on the router and USB hub light up. Additionally, you can now boot the NVIDIA Xavier by pressing the left-most button on the back of the unit. After allowing approximately 1-2 minutes for everything to boot up, you can start to connect to the car. The WIFI SSID for a given car should be RACECAR_MX_# where # is the number of your car (i.e. 0 , 1 , 2 , or 3 ). Your car, Xavier, controller, battery, and router should all be labeled to help you determine your car number. The WIFI password should be the familiar g0_fast! . Once connected to the WIFI, you should be able to SSH to the car with the following command: ssh racecar@192.168.0.20# again where # is the number of your car. Again, the password should be the familiar racecar@mit . Once connected to your car, you should notice a similar file structure to the MT car. One thing to note is the NVIDIA Xaviers exclusively run Ubuntu 18.04. The TX2 you have been using on the MT cars are still using Ubuntu 16.04. While this should not matter for the labs, it is worth noting in case you try to install additional packages.","title":"Task 1: Boot Up"},{"location":"gate_detection/#task-2-teleop-launch-files","text":"Once connected to your car, we will prepare to teleop the car for the first time. Before doing this, make sure to connect your Traxxas battery to the VESC and give it sufficient time to boot up and connect to the Xavier as a serial device. Run the command: teleop Recall this is an alias defined in ~/.bashrc for the more verbose command: roslaunch racecar teleop.launch What do you see? Does the car drive? Is anything wrong? Screenshot of teleop running on a fresh MX car Likely, you will observe the car teleops as expected, but your terminal is polluted with errors. This is because by default, the MX car is configured with the same software stack as the standard MT RACECAR. Like all things, this has both pros and cons. The pro here is this is identical code to what you have been using all semster (i.e. the code structure, rostopics, etc.). The con here is that the out-of-the-box defaults are not ideal for the MX car. Upon examining the error messages, you will notice they are in response to missing sensors. Specifically, the LiDAR and IMU (the ZED did run by default from teleop ). Modify your launch files to a configuration that makes sense for the MX car. (Hint: This will just involve removing a few lines to not run drivers for missing sensors.)","title":"Task 2: Teleop &amp; Launch Files"},{"location":"gate_detection/#task-3-test-cameras","text":"Take a look at the simple Python file ~/webcams.py in your home directory. What do you think it does? Does it run? Screenshot of python ~/webcams.py without running ssh with a -X argument Likely, the program does not run since there is a GUI component. Exit your ssh connection with the car and rejoin with the command ssh -X racecar@192.168.0.20# again where # is the number of your car. The -X argument enables X11 forwarding (i.e. allows you to render remote GUI components on your local machine). Try running the script again. Does it run now? Is there delay? Screenshot of python ~/webcams.py while running ssh with a -X argument You can expect to see delay in the frames. This is simply because you are streaming uncompressed images. The vehicle should be receiving these images on time. Are the cameras aligned accross the vertical dimenson? What happens if we remove the frame dimension restrictions in software? Why might this happen? Are the cameras displaying in the right order? Fix the camera ordering in software if needed.","title":"Task 3: Test Cameras"},{"location":"gate_detection/#task-4-write-ros-node","text":"Finally, we will write our very own ROS Node for our accessing these pictures within the ROS ecosystem. Using ~/webcams.py as a starting point, modify the code to produce a useful rostopic interface. You will be using this interface for the rest of these NEET Labs. (Hint: Recall how the /zed/ topics were structured. Would something similar work?) Once your ROS Node is complete and tested, put the file in a sensible place within the ~/racecar_ws file hierarchy. Do not add it to your launch script. You will want to selectively run this script in the future.","title":"Task 4: Write ROS Node"},{"location":"gate_detection/#part-2-gate-detection","text":"In this lab we will be creating a weakly-supervised deep learning system for detecting gates in TensorFlow. This lab is inspired by the methodology outline in a paper presented at CVPR 2015 called Is object localization for free? - Weakly-supervised learning with convolutional neural networks If you find this material interested, we highly encourage you to read the original paper.","title":"Part 2: Gate Detection"},{"location":"gate_detection/#background-on-ml","text":"The following Topic Sections will review some fundamental machine learning topics neccessary for this lab. There are no tasks in this section. If you are already familiar and comfortable with these topics, feel free to jump to the Task Sections and reference this information as needed. This background material is adapted from a previous lab offered in section of 6.a01. [Source]","title":"Background on ML"},{"location":"gate_detection/#topic-1-what-is-a-tensor","text":"Tensor is a term that gets thrown around a lot these days in the context of machine learning. For instance, TensorFlow, the machine learning library we will be using for the remainder of the NEET assignments, gets its namesake from this concept. Put simply, a tensor is just a generalization of vector, matrix, etc. A scaler is a 0-dimensional tensor, a vector is a 1-dimensional tensor, a matrix is a 2-dimension tensor, etc. The rank of a tensor is simply how many dimensions the tensor has. Similarly, a scaler is rank 0, a vector is rank 1, a matrix is rank 2, etc. In this lab, the input to our neural network will be color images coming from the webcams in Part 1. Can you guess the rank and shape of the tensor representation? (Hint: the dimensions are ordered as [height, width, channels] ). There are many different color spaces used to represent digital photos. The most common is RGB. If you said, [240, 320, 3] , you are absolutely correct! Keeping this straight is critical but also challenging (especially since some libraries alternate between wanting coordinates in (x,y) and other times in (r,c) . This is further complicated when you group multiple images into dataset. A group of these images is called a batch , has a rank of 4, and ordered dimensions of [batch_size, height, width, channels] . When you start constructing your own networks, you will notice many bugs boil down to mismatching the of the previous layer's output dimensionality to the next layer's input dimensionality. You can always check the dimensionality of a tensor by printing out the shape property in Python. >>> print(batch.shape) (6000,240,320,3)","title":"Topic 1: What is a Tensor?"},{"location":"gate_detection/#topic-2-what-are-convolutions-and-their-uses","text":"A convolution is a mathematical operation with its roots in functional analysis. Although this is the same discrete operation found in probability, statistics, signal processing, etc., here you can primarily think of it as an operation that changes the shape of a tensor through a small number of learned weights. Visually, this can be interpreted as the following diagram: The input image (bottom, blue) is undergoing a convolution to produce the output (top, teal). A small patch of learned weights is slid over every location. At each position, the weights are multiplied with the overlapping input image values and then all summed together to make a new single value pixel. At a given location, this is governed by the equation: In the above gif, there are 9 weights ( w_1 , w_2 , w_3 , ... , w_9 ) and 9 input values ( x_1 , x_2 , x_3 , ... , x_9 ). There is also a bias term, b , that you can disregard in this example since it is 0. Althought the specific mathematical details are beyond the scope of this tutorial, a trained convolutional layer is useful because it learns to extract useful localized features from input data. The first convolutions (bottom layer) recognizes basic features like corners or lines. The next layer (middle layer) combine these features into primitive shapes such as eyes and ears. The last layer (top layer) combines these features into complete images such as faces. Layering convolutions is what builds this feature higherarchy and is what makes deep learning deep. Additionally, in our use case we are interested in determining where an object is in an image. One useful feature of convolutions is that they do not act over the entire image, but on small subsections. Even after multiple convolutions, the top left pixel of the output image is only going to give us information about the top left patch of the input image. If we use convolutions for classification we are actually training a whole bunch of classifiers in parallel. Each pixel of the output will tell us whether an object is located in a particular portion of the image. We can then use this heatmap to detect where our object of interest resides. Warmer pixels indicate regions of the picture that strongly impacted the classification of the overal picture.","title":"Topic 2: What are Convolutions and their Uses?"},{"location":"gate_detection/#task-1-tensorflow-tutorials","text":"Before jumping into using TensorFlow to detect gates, we will first complete a useful tutorial on TensorFlow. This is because, while an industry staple, TensorFlow can have a steep learning curve. This learning curve is well worth it due to TensorFlow's unmatched power and featureful ecosystem. Also, recent supplimental libraries, like Keras, have greatly reduced the complexity of common ML tasks. You may complete this tutorial as a team or individually. The remainder of the NEET labs will build on this fundamental TensorFlow library knowledge, so make sure everyone understands the concepts individually if you choose to work as a group. When completing this tutorial you may run it locally on your personal laptop or remotely from your MX car. We suggest you work remotely on your car since TensorFlow is already installed there. If you want to work locally, you may follow the TensorFlow installation instructions here . To best mimic the tutorial and the MX, we recommend you complete the tutorial using Python3 (TensorFlow on the Xavier currently only works with Python3). Additionally, we recommend you complete the tutorial in a Jupyter Notebook , a handy interactive Python workspace served through a web brower. To configure the Jupyter Notebook server on your MX RACECAR, complete the following steps: After running SSH to access your MX car, confirm the necessary libraries are installed/install them with: sudo apt install jupyter-notebook sudo apt install python3-matplotlib Generate a default server configuration file with the following command: jupyter notebook --generate-config Edit the generated default config file, /home/racecar/.jupyter/jupyter_notebook_config.py , for remote access by uncommenting the following variables and modifying their values: c.NotebookApp.allow_remote_access = False c.NotebookApp.ip = '192.168.0.20X' # where X is the number of your car c.NotebookApp.open_browser = False c.NotebookApp.port = 9999 Make sure you edit the IP address to include your car's number when modifying the config file. If you are interested in further configuring the server, including setting a permanent password, check out the documentation here . Navigate to a working directory of your choice, perhaps ~/tf-tutorial and start the server with the command: jupyter-notebook Among the server console output will URL of the form: http://192.168.0.200:9999/?token=55db505149ca8e1efd2293455fd0ad97a3220e1e084c881a While connected to the car's router, copy this URL into a web browser on your local machine. You should see a web page similar to the picture below. Note: In the future, if you want to use Jupyter Notebooks on your car again, you should only have to repeat steps 4 and 5.","title":"Task 1: TensorFlow Tutorials"},{"location":"gate_detection/#task-2-collecting-data","text":"Now that you are familiar with the basics of TensorFlow, it is time to start building your own gate detection solution. As you may have already began to realize, machine learning can only ever be as good as the data you collect. Thus, in this phase, we're going to get some practice collecting data. Luckily, this is a relatively simple machine learning task so you will not need to collect too much data. Additionally, since this solution is weakly supervised, you will only need to have data labeled as \"contains gate\" and \"does not contain gate\" -- no bounding boxes required. For reference, a TA solution was completed with 4800 320x240 pictures from the front camera only: 2000 training images containing gates 2000 training images not containing gates 200 verification images containing gates 200 verification images not containing gates Data was collected at 10 frames/sec, meaning the whole data collect took approximately 8 minutes. You are welcomed and encouraged to modify these quantities, use the side cameras, etc. Just know if you are spending hours collecting data, something is wrong and you should reach out to us on Slack. Example gate image from collected dataset. To collect data, feel free to modify the simple script record.py included in this repository. Materials for the gates will be left in the cage. Contact a TA if you need access. Additionally, you can examine specific image files collected with the handy tool Eye of GNOME (or eog for short). You can use the tool like this: eog [FILENAME] You can also cycle through images in that directory with the arrow keys and delete individual images with the Delete key. Since this is a GUI tool, make sure you are running your ssh session with the -X argument.","title":"Task 2: Collecting Data"},{"location":"gate_detection/#task-3-training-a-model","text":"We will start by creating a simple gate classifier. This network will be trained on the data you collected and output a binary classification -- 0 for no gate, 1 for gate. To get started, open up and read through the code in Train.ipynb . You will notice much of this code is reminiscent, and in fact adapted from, the tutorial you completed on the MNIST dataset. There are two primary modifications you must make to this file: 1. Modify the helper function load_data() to properly load the data you collected. 2. Construct a neural network within the # build model structure block. Try a handful of Conv2D layers and end with a handful of Dense layers. Mix a few AveragePooling2D and Flatten layers in between. Most important is to experement. If you're stuck, try implementing Lenet (pictured below) and modify it from there. Which is preforming best on your validation data? For reference, you should shoot for 95% or more accuracy on your training set and 80% or more accuracy on your validation set. The final step of this notebook saves your trained model for use in the next task.","title":"Task 3: Training a Model"},{"location":"gate_detection/#task-4-testing-a-model","text":"Now that we have a trained model, we will use it on data taken directly from the webcam and produce a heatmap. To do this, use Heatmap.ipynb . If your model from the previous task worked properly, you can expect to: 1. Correctly classify the picture you take with the webcam as gate or no_gate 2. In a photo of a gate, generate a heatmap that highlights the gate or critical features of the gate. (Note: Although portions may be highlighted in pictures without a gate as long as the entire image is correctly classified as no_gate you will disregard these heatmaps.) Once you have your heatmap, implement a way to generate a steering angle from the heatmap. Consider methods that have you find the center of the high activate regions. Prototype it in the Jupyter Notebook before we integrate it with ROS in the next Task.","title":"Task 4: Testing a Model"},{"location":"gate_detection/#task-5-integrating-with-mx","text":"This step is, unfortunantely, slightly more complicated than ideal due to compatibility issues. Specifically, the Xavier will only run TensorFlow in Python3 while ROS will only run in Python2. As a result, we need to circumvent this by having a network connection between a session of Python3 and Python2. Although the exact architecture is up to you, we recommend having the Python3 as a server responsible for producing steering angles and Python2 as a client responsible for turning a steering angle into a ROS message. With this method, you will copy a bulk of your code from the Jupyter Notebook into your server script (ideally a .py file, not another .ipynb Jupyter Notebook). With this architecture, the server will include code that connects to the webcam, fetchs a picture, runs it through TensorFlow, produces a heatmap, analyzes the heatmap to produce a steering angle, and, when queried by a client, responds with the current steering angle. To handle the networking, we recommend using ZMQ. You can checkout some documentation here . We've also included a sample server and client program in this repo, originating from the ZMQ documentation. hwserver.py hwclient.py Additionally, this documentation shows how to easily define two functions send_array() and recv_array() transfer a np.array between two programs. Note: ZMQ uses a REQ/REP (request/respond) Lockstep Protocol to send messages. For using this library, just make sure each program alternates between sending/receving messages. Otherwise, if you try to send or receive two messages in a row, you'll get an error about the context not being valid.","title":"Task 5: Integrating with MX"},{"location":"imitation_learning/","text":"Imitation Learning Lab Introduction This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to regress a steering angle directly from an image taken from the \"front bumper\" of a car. Here, \"imitation learning\" refers to a branch of supervised machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. This learning objective is also frequently termed behavioral cloning. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann steering controller) from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will regress similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement. In simulation: Lake Track Jungle Track In Stata basement: This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions Part 1: Install required Python libraries and the simulation environment TensorFlow, a deep-learning framework You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Note Even though you will be installing miniconda-python-3.7 , we will be using Python 2.7 to define and train the PilotNet CNN model. miniconda-python-3.7 will handle creating a Python 2.7 environment for you. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2.7 ROS environment on the RACECAR. (To note for completeness, is also possible to train a model with Python 3 and import it with Python 2) Once you have installed miniconda, clone the following repository locally: $ git clone https://github.com/mit-alphapilot/imitation_learning_lab $ cd imitation_learning_lab/ Next, we will install TensorFlow using the conda command. There are two options: If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml Udacity self-driving-car simulator Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64 Part 2: Defining the PilotNet model Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet CNN model expects 200x66 crops from the car's camera. Exercise How many trainable parameters are in this model? What is the output volume of each layer? What is the effect of changing the input size on the total number of parameters in the model? Hint 1: use model.summary() to print out a summary of the network. Hint 2: Consider the input to the flattening operation and first dense layer: it is the output volume from the last convolutional layer. How is this affected by changing the input size? What about the next dense layer? For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details. Model Output and Optimization The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the steering angle training data to fit between (-1, 1), and therefore we should expect regressions from the CNN to also fit between this range. Question Notice that we also normalize the input images in the first layer between (-1,1) Why would we prefer to normalize the input and output data between these ranges? Hint: Consider the shape, domain, and range of common activation functions. We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise With only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well. This might help your team to complete the course faster! For instance, when you are collecting training data, you might want to drive quickly down straight hallways and slow down during turns. It is feasible to learn this behavior! Part 3: Training the Model We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet: Jupyter Notebook We will train our models in Jupyter Notebook: $ conda activate imitation_learning (imitation_learning) $ cd imitation_learning_lab (imitation_learning) $ jupyter notebook Then, open train_RACECAR_pilotnet.ipynb in your browser. In Simulation First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg Train/Validation Split Regularization With enough training time and enough model parameters, you can perfectly fit your training data! This is called overfitting - we will use validation data, image augmentation, and regularization to avoid overfitting. We will partition our data into training and validation sets. Validation helps to ensure your model is not overfitting on the training data. In the notebook, observe the use of from sklearn.model_selection import train_test_split. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED) Batch Generation, Checkpointing, and Training Execution For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1) Driving your car in simulation After starting the Udacity simulator in Autonomous Mode you can use your trained model to drive the car via: $ python drive_udacity.py $MODEL_NAME where $MODEL_NAME is the name of the saved model checkpoint. For instance, if you used the following checkpoint naming scheme: ModelCheckpoint('imitationlearning-{epoch:03d}.h5' ...) Then imitationlearning-010.h5 will be the model saved after the tenth epoch of training. Image Augmentation You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle Servo histograms It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above: Checkpointing checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') [Optional] Extending to more general environments It is possible to train a model with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! You can find useful public road data from Udacity here: https://github.com/udacity/self-driving-car/tree/master/datasets Another useful public road dataset is here: https://github.com/SullyChen/driving-datasets Part 4: RACECAR data collection and training In this section you will manually collect steering angle data by driving the car around. A good first task is to train the RACECAR to drive around some tables in a circle, before tackling Stata basement. You can also define some more intermediate-difficulty courses (figure-eights, snake patterns, etc) to gain intuition on what types of training and validation methods are most effective. Here is an example of training data collected around some tables in a classroom: And here is a third-person view of a car autonomously driving around the same path using PilotNet: The following script will record images from the three webcams on the RACECAR along with the joystick-commanded steering angle (through teleop ): https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/record_RACECAR.py $ python2 record_RACECAR.py Note Make sure to use python2 here for recording the images and steering angle data - note that we will not use python2 in the next section to access the cameras. TensorFlow is only available in the car's python3 environment, and ROS is only available in our python2 environment. For recording, we do not need access to TensorFlow, only OpenCV. For autonomous driving with both TensorFlow and ROS, we will see how to work around this inconvenience in the next section via zmq . When you are done collecting data, press ctrl-c to terminate collection: Note After you are done with data collection, you might see an error like TypeError: img data type = 17 is not supported printed to the terminal when you ctrl-c - this is harmless and you can safely ignore this error, which happens because the last jpg was not saved to disk due to the ctrl-c interrupt. The rest of the jpgs should be in the saved folder along with a CSV file of jpg filenames, steering angles, and velocities. Note that before recording any data you will need to change the Video Device IDs to the appropriate values, depending on which order the webcams were plugged in and registered by Linux. Set the appropriate values for your car in camera_RACECAR.py class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 The following script will display the currently assigned video device IDs on top of the camera feeds, to help verify the IDs are in the correct order: $ python3 video_id_RACECAR.py python2 should also work above. Sidenote: you can also set udev rules to \"freeze\" these values for your car, if you frequently find the IDs changing after power-cycling the RACECAR. After you have collected your training data, transfer the data using scp or a flashdrive to your laptop and train your model using the provided jupyter notebook . Reminder You should not train a model on the RACECAR Part 5: Running inference on RACECAR To execute a trained model, you will need to run the following scripts: https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/infer_RACECAR.py https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/drive_RACECAR.py Note We use zmq to send steering angle messages from our Python3 script running inference with TensorFlow, over to a Python2-ROS script that commands the car to drive. You will first need to copy your saved model weights to the RACECAR (e.g., using SCP). You will specify the model location using this command-line argument . Next, if it has changed (due to a reboot or unplugging the cameras), remember to modify the video ID to the center camera here , or verify the current ID is correct using video_id_RACECAR.py : class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 Note We only need the center camera during inference. Then, in one terminal, run: $ python3 infer_RACECAR.py --model path_to_model.h5 Ensure you are using python3 above. In another terminal, use python2 and run: $ python2 drive_RACECAR.py Optional exercise This script also includes a mean filter. You can remove this, extend or shorten the length of the mean filter, change it to a median filter, etc, to experiment with inference behavior while driving. visualize_drive.ipynb can be used to overlay steering angle predictions on top of saved runs (see infer_RACECAR.py for a flag that saves images during inference): Tips and Suggestions If you are having diffuclty training a working model, here are some suggestions: Visualize your model's predictions by saving the center camera images when you are testing a model (you can use the SAVE_RUN flag) and incorporate some of the code snippets from visualize_drive.ipynb . Are the model outputs noisy (i.e., are the predicted angles jumping around a lot)? Try using the mean or median filter in infer_RACECAR.py . Are the inference angles wrong? Find some images in your training data that come from a similar point in the track where your inferred angles are wrong - how do the model's predictions look there on the training data? If they are also bad, you can try to collect more data in that spot on the track (for instance, you can reset the car to the starting position of a corner several times and record several turns). In addition to visualizing the model's predictions on that section of the track in your training data, also inspect the images and steering angles in the CSV at that point in the track - maybe the car was not being driven smoothly at that location when collecting training data. The dimensions of the model inputs will have a significant impact on the model's accuracy, speed of evaluation (i.e., can you run inference with the model at 30FPS, 15FPS, etc.), and training time/data required (a larger input creates more parameters which may take more training time). How large of an input will you use? The original resolution of the camera frame? Or will you downscale the images? How much will you crop out of the input images? You can choose vertical or horizontal amounts independently of each other. Or you can try using the full camera frame. The number of distinct, unique visual features at each point in the track (i.e., higher input resolutions and larger crops of each frame will include more details from the walls, ceiling, and floor) used when training the model will impact how well the model can memorize the correct steering output to predict at that point in the track during inference, but this will require more training time and data. Smaller inputs can help make the model more robust and generalizable, and reduce the amount of training time. Make sure you can train a model that works on a smaller environment (e.g., around a couple of tables or around a classroom) before tackling the full Stata basement loop. Remember: the training and validation errors are not a great indicator of how well the model wll drive, compared to testing model variants on the car. They are a better indicator of whether the model is continuing to fit (i.e., \"learn\"). Be wary of overfitting: try multiple saved checkpoints instead of just the last one (a checkpoint is saved every epoch). You can shorten the number of training steps per epoch and increase the number of epochs to have more models to try out. You can also try a larger batch size. Try to plan multiple training experiments ahead of time. Instead of changing one hyperparameter, training a model and testing, and going back to change another hyperparameter, try to train several models with different hyperparameters in one session and copy them all over to your racecar. Moreover, if you are using the SAVE_RUN flag, you can try visualizing predictions from all your model variants on the same saved run - you might find another trained model or saved epoch is doing better than the one you were using for testing. You can try adding more model regularization (e.g., more dropout or batchnorm layers) to avoid overfitting. You might also try to decrease the learning rate (you will need to train for longer) - if the learning rate is too high, the model will coverge too quickly. You might also choose to experiment with different activation types (ReLU, ELU, Tanh, ...) since they have different characteristics with respect to backprop. Try training on only the center camera and see if that model performs better. If it does perform better, you might have a poor value selected for your OFFSET_STEERING_ANGLE - it may either be too small or too large, depending on your camera's positioning. Some manually-driven tests may help you to find a better value (every car handles a bit differently). Try significantly increasing the amount of overlap between the three cameras. Make sure they are all level with each other if you re-position the cameras. You will need to unaffix and retape your cameras down while looking at the camera stream (using x-forwarding over ssh). Though this might reduce the chances of your car recovering if it makes a bad turn, it will effectively triple the amount of data you are collecting along the nominal path. If you do this, remember to reduce your OFFSET_STEERING_ANGLE value. With x-forwarding to view the live camera stream, or when copying training data from the RACECAR to your laptop or a server, connecting to the car's router via Ethernet will probably be faster than connecting over WiFi. If you do need to stream or copy data over WiFi, try to use the 5GHz network which will probably be faster. Read Nvidia's PilotNet paper and papers which have cited it (e.g., via Google Scholar) for many more ideas (often in methodology sections, and future work discussions). One powerful regularization/generalization technique for training neural networks is multi-task learning . A crude version of this approach might be to add a second output to the network which predicts velocities (also present in your CSV). It is optional whether you choose to command the model's predicted velocities or continue to command a fixed speed (the current behavior of drive_RACECAR.py ) when running inference - the benefits to model regularization may still be gained from this change. However, this will likely require more training data.","title":"Imitation Learning Lab"},{"location":"imitation_learning/#imitation-learning-lab","text":"","title":"Imitation Learning Lab"},{"location":"imitation_learning/#introduction","text":"This lab provides an introduction to end-to-end imitation learning for vision-only navigation of a racetrack. Let's break that down: We will train a deep learning model - specifically, a convolutional neural network (CNN) - to regress a steering angle directly from an image taken from the \"front bumper\" of a car. Here, \"imitation learning\" refers to a branch of supervised machine learning which focuses on imitating behavior from human-provided examples. In our case, we will drive a car around a track several times to provide examples for the CNN to mimic. This learning objective is also frequently termed behavioral cloning. We will contrast this with our next lab on \"reinforcement learning\" where a robot agent learns to accomplish a goal via exploration , not via examples. \"Vision-only\" refers to using an RGB camera as the only input to the machine learning algorithm. LIDAR, depth, or vehicle IMU data are not used. Here, \"end-to-end learning\" is shorthand for the CNN's ability to regress a steering angle (i.e., an actuation for the Ackermann steering controller) from unprocessed input data (pixels). We will not need to pre-process input features ourselves, such as extracting corners, walls, floors, or optical flow data. The CNN will learn which features are important, and perform all the steps from image processing to control estimation itself (\"end-to-end\", loosely speaking). We will start by driving a simulated car around a virtual racetrack and collecting camera data from the rendered game engine, as well as our game inputs. We will define a CNN that will regress similar game inputs in order for the car to complete the same track autonomously. Next, we will train the model using camera data and steering angles collected from the RACECAR platform in a real-world environment, the basement in Stata Center, in order for the RACECAR to autonomously drive through the Stata basement.","title":"Introduction"},{"location":"imitation_learning/#in-simulation","text":"Lake Track Jungle Track","title":"In simulation:"},{"location":"imitation_learning/#in-stata-basement","text":"This lab and the CNN architecture we will use are based on PilotNet from Nvidia: Nvidia's blog post introducing the concept and their results Nvidia's PilotNet paper Udacity's Unity3D-based Self-Driving-Car Simulator and Naoki Shibuya's drive.py contributions","title":"In Stata basement:"},{"location":"imitation_learning/#part-1-install-required-python-libraries-and-the-simulation-environment","text":"","title":"Part 1: Install required Python libraries and the simulation environment"},{"location":"imitation_learning/#tensorflow-a-deep-learning-framework","text":"You first need to install miniconda to install TensorFlow. Download the Python 3.7 version of miniconda and follow the installation instructions for your platform. Note Even though you will be installing miniconda-python-3.7 , we will be using Python 2.7 to define and train the PilotNet CNN model. miniconda-python-3.7 will handle creating a Python 2.7 environment for you. Once we save a trained model (also known as saving weights ), we can later import the saved model in a Python 2.7 ROS environment on the RACECAR. (To note for completeness, is also possible to train a model with Python 3 and import it with Python 2) Once you have installed miniconda, clone the following repository locally: $ git clone https://github.com/mit-alphapilot/imitation_learning_lab $ cd imitation_learning_lab/ Next, we will install TensorFlow using the conda command. There are two options: If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml","title":"TensorFlow, a deep-learning framework"},{"location":"imitation_learning/#udacity-self-driving-car-simulator","text":"Download the Udacity Term 1 simulator for your platform: Linux Mac Windows The full Unity3D source code of this simulator is available here , as well as other simulators for LIDAR data, waypoint-following, traffic, etc. We will only be using the simulator linked above. Extract the simulator (which will create a folder called beta_simulator_linux/ ): $ unzip term1-simulator-linux.zip On Linux, you will need to make the simulator executable, via the chmod command: $ chmod +x ./beta_simulator_linux/beta_simulator.x86_64","title":"Udacity self-driving-car simulator"},{"location":"imitation_learning/#part-2-defining-the-pilotnet-model","text":"Let us take a closer look at the CNN architecture for PilotNet: In this lab, we will command a fixed driving velocity and only regress steering angles from images using PilotNet. Hence, the PilotNet CNN has a single output. Using TensorFlow's Keras API , let us look at an implementation of the above network in code: from tensorflow.keras.layers import Lambda, Conv2D, MaxPooling2D, Dropout, Dense, Flatten from tensorflow.keras.models import Sequential # you will need to crop or shrink images to the dimensions you choose here: IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 66, 200, 3 INPUT_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) def build_model(dropout_rate=0.5): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) #normalizes image data model.add(Conv2D(24, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Conv2D(64, (3,3), activation='elu')) model.add(Dropout(dropout_rate)) model.add(Flatten()) model.add(Dense(100, activation='elu')) model.add(Dense(50, activation='elu')) model.add(Dense(10, activation='elu')) model.add(Dense(1)) model.summary() return model As configured above, the PilotNet CNN model expects 200x66 crops from the car's camera. Exercise How many trainable parameters are in this model? What is the output volume of each layer? What is the effect of changing the input size on the total number of parameters in the model? Hint 1: use model.summary() to print out a summary of the network. Hint 2: Consider the input to the flattening operation and first dense layer: it is the output volume from the last convolutional layer. How is this affected by changing the input size? What about the next dense layer? For more on TensorFlow's Keras API, click here . Note Note that Keras will disable Dropout regularization at inference time. See here for details.","title":"Part 2: Defining the PilotNet model"},{"location":"imitation_learning/#model-output-and-optimization","text":"The output of this model is a single neuron, which corresponds to the servo or steering angle to command the car with. In the section on Training we will normalize the steering angle training data to fit between (-1, 1), and therefore we should expect regressions from the CNN to also fit between this range. Question Notice that we also normalize the input images in the first layer between (-1,1) Why would we prefer to normalize the input and output data between these ranges? Hint: Consider the shape, domain, and range of common activation functions. We will use the Adam optimizer with a loss function (i.e., cost or objective function) that minimizes the mean square error betwen the ground-truth steering angles and the currently predicted steering angles: model = build_model() model.compile(loss='mean_squared_error', optimizer=Adam(lr=1.0e-4)) Optional Exercise With only a few changes to the above model and loss definitions, you can add a second output to estimate the velocity as well. This might help your team to complete the course faster! For instance, when you are collecting training data, you might want to drive quickly down straight hallways and slow down during turns. It is feasible to learn this behavior!","title":"Model Output and Optimization"},{"location":"imitation_learning/#part-3-training-the-model","text":"We will use three cameras mounted on the simulated car and the real-world RACECAR to collect training data. This excerpt from Nvidia's blog post explains why doing so is useful: Training data contains single images sampled from the video, paired with the corresponding steering command (1/r). Training with data from only the human driver is not sufficient; the network must also learn how to recover from any mistakes, or the car will slowly drift off the road. The training data is therefore augmented with additional images that show the car in different shifts from the center of the lane and rotations from the direction of the road. The images for two specific off-center shifts can be obtained from the left and the right cameras. Additional shifts between the cameras and all rotations are simulated through viewpoint transformation of the image from the nearest camera. Precise viewpoint transformation requires 3D scene knowledge which we don\u2019t have, so we approximate the transformation by assuming all points below the horizon are on flat ground, and all points above the horizon are infinitely far away. This works fine for flat terrain Here is a diagram from Nvidia that describes the training and data augmentation process for PilotNet:","title":"Part 3: Training the Model"},{"location":"imitation_learning/#jupyter-notebook","text":"We will train our models in Jupyter Notebook: $ conda activate imitation_learning (imitation_learning) $ cd imitation_learning_lab (imitation_learning) $ jupyter notebook Then, open train_RACECAR_pilotnet.ipynb in your browser.","title":"Jupyter Notebook"},{"location":"imitation_learning/#in-simulation_1","text":"First, create a new folder to store training data from the simulator, e.g. training/ ) and then start the simulator. On linux: $ ./beta_simulator_linux/beta_simulator.x86_64 Now launch the Training mode and configure the simulator to save data to the folder you created: Once you have configured a folder to record your training data into, press record again (or r as a shortcut) and start to drive the car around (you can use WASD or your arrow keys on your keyboard). If this is your first time running the simulator, stop recording after a few seconds, to inspect the saved results. The simulator will save three camera views from the car: left, center, and right camera views from the bumper of the car (as jpgs) along with the image filenames and the current steering angle in driving_log.csv . Warning Recording will generate a lot of files! Too many files in a single directory can cause performance issues, e.g., when generating thumbnails. Once you are ready to record full laps of the course, I recommend keeping each recording session to a few laps of the track, and making multiple new folders. This will help to keep the number of files within each folder low, e.g., two_laps_run1/ , two_laps_run2/ , etc, or making a folder for tricky sections of the course, e.g., bridge_section_run1/ . It is easy to concatenate the resulting CSVs in python (using simple list concatenation with + ) In the training/ folder (or whichever folder you just created), you should see driving_log.csv and another folder IMG/ . driving_log.csv contains path locations for the three camera views with associated timestamps (sequentially increasing), and the saved steering angle (without a CSV header): Center Left Right Steering Angle Throttle Brake Speed center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg 0 0 0 0 center_2019_03_11_12_22_15_502.jpg left_2019_03_11_12_22_15_502.jpg right_2019_03_11_12_22_15_502.jpg 0 0 0 0 center_2019_03_11_12_22_15_594.jpg left_2019_03_11_12_22_15_594.jpg right_2019_03_11_12_22_15_594.jpg 0 0 0 0 Note If you would like to change some of the parameters, such as the saved image resolution, or even define a new track, you can rebuild and edit the simulator using Unity3D . See this section of the Udacity source code if you are curious how the image files and CSV data are generated. In the training/IMG/ folder you will find .jpg files with the following naming scheme corresponding to the above CSV: Left Center Right center_2019_03_11_12_22_15_385.jpg left_2019_03_11_12_22_15_385.jpg right_2019_03_11_12_22_15_385.jpg","title":"In Simulation"},{"location":"imitation_learning/#trainvalidation-split","text":"Regularization With enough training time and enough model parameters, you can perfectly fit your training data! This is called overfitting - we will use validation data, image augmentation, and regularization to avoid overfitting. We will partition our data into training and validation sets. Validation helps to ensure your model is not overfitting on the training data. In the notebook, observe the use of from sklearn.model_selection import train_test_split. imgs = [] angles_rad = [] #normalize between -pi to pi or -1 to 1 with open(driving_data) as fh: ########################## # ### TODO: read in the CSV # ### and fill in the above # ### lists # ############################ TEST_SIZE_FRACTION = 0.2 SEED = 56709 # a fixed seed can be convenient for later comparisons X_train, X_valid, y_train, y_valid = train_test_split( imgs, angles_rad, test_size=TEST_SIZE_FRACTION, random_state=SEED)","title":"Train/Validation Split"},{"location":"imitation_learning/#batch-generation-checkpointing-and-training-execution","text":"For efficient training on a GPU, multiple examples are sent at once in a batch onto the GPU in a single copy operation, and the results of backpropagation are returned from the GPU back to the CPU. You will want to checkpoint your model after each epoch of training. Lastly, model.fit_generator() will commence training on your data and display the current loss on your training and testing data: checkpoint = ModelCheckpoint('imitationlearning-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto') def batch_generator(image_paths, steering_angles, batch_size): \"\"\" Generate training image give image paths and associated steering angles \"\"\" images = np.empty([batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS]) steers = np.empty(batch_size) while True: i = 0 for index in np.random.permutation(len(image_paths)): ############################################## # TODO: add your augmentation code here #### ############################################## image = cv.imread(image_paths[index]) cropped = image[95:-95, 128:-127, :] images[i] = cropped steering_angle = steering_angles[index] steers[i] = steering_angle i += 1 if i == batch_size: break yield images, steers BATCH_SIZE=20 model.fit_generator(generator=batch_generator(X_train, y_train, batch_size=BATCH_SIZE), steps_per_epoch=20000, epochs=10, validation_data=batch_generator(X_valid, y_valid, batch_size=BATCH_SIZE), # https://stackoverflow.com/a/45944225 validation_steps=len(X_valid) // BATCH_SIZE, callbacks=[checkpoint], verbose=1)","title":"Batch Generation, Checkpointing, and Training Execution"},{"location":"imitation_learning/#driving-your-car-in-simulation","text":"After starting the Udacity simulator in Autonomous Mode you can use your trained model to drive the car via: $ python drive_udacity.py $MODEL_NAME where $MODEL_NAME is the name of the saved model checkpoint. For instance, if you used the following checkpoint naming scheme: ModelCheckpoint('imitationlearning-{epoch:03d}.h5' ...) Then imitationlearning-010.h5 will be the model saved after the tenth epoch of training.","title":"Driving your car in simulation"},{"location":"imitation_learning/#image-augmentation","text":"You will want to add some data augmentation to help your model generalize past the specific examples you have collected in the simulator (and on the actual RACECAR). Some example transformations to incorporate: Center Image Left and right Images def choose_image(data_dir, center, left, right, steering_angle): \"\"\" Randomly choose an image from the center, left or right, and adjust the steering angle. \"\"\" choice = np.random.choice(3) if choice == 0: return load_image(data_dir, left), steering_angle + 0.2 elif choice == 1: return load_image(data_dir, right), steering_angle - 0.2 return load_image(data_dir, center), steering_angle Flipped Image if np.random.rand() < 0.5: image = cv2.flip(image, 1) steering_angle = -steering_angle return image, steering_angle Translated Image def random_translate(image, steering_angle, range_x, range_y): \"\"\" Randomly shift the image virtially and horizontally (translation). \"\"\" trans_x = range_x * (np.random.rand() - 0.5) trans_y = range_y * (np.random.rand() - 0.5) steering_angle += trans_x * 0.002 trans_m = np.float32([[1, 0, trans_x], [0, 1, trans_y]]) height, width = image.shape[:2] image = cv2.warpAffine(image, trans_m, (width, height)) return image, steering_angle","title":"Image Augmentation"},{"location":"imitation_learning/#servo-histograms","text":"It is important to ensure the train/test split of the data you have collected have similar driving condition represented. For instance, here is the histogram of servo angles in the training and testing data used above:","title":"Servo histograms"},{"location":"imitation_learning/#checkpointing","text":"checkpoint = ModelCheckpoint('model-{epoch:03d}.h5', monitor='val_loss', verbose=0, save_best_only=False, mode='auto')","title":"Checkpointing"},{"location":"imitation_learning/#optional-extending-to-more-general-environments","text":"It is possible to train a model with driving data from public roads, in order to experiment with how it affects the performance of your car in Stata basement. Danger Obviously, you should not test anything on public roads yourself, either on a RACECAR or any other car. Be safe and responsible! You can find useful public road data from Udacity here: https://github.com/udacity/self-driving-car/tree/master/datasets Another useful public road dataset is here: https://github.com/SullyChen/driving-datasets","title":"[Optional] Extending to more general environments"},{"location":"imitation_learning/#part-4-racecar-data-collection-and-training","text":"In this section you will manually collect steering angle data by driving the car around. A good first task is to train the RACECAR to drive around some tables in a circle, before tackling Stata basement. You can also define some more intermediate-difficulty courses (figure-eights, snake patterns, etc) to gain intuition on what types of training and validation methods are most effective. Here is an example of training data collected around some tables in a classroom: And here is a third-person view of a car autonomously driving around the same path using PilotNet: The following script will record images from the three webcams on the RACECAR along with the joystick-commanded steering angle (through teleop ): https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/record_RACECAR.py $ python2 record_RACECAR.py Note Make sure to use python2 here for recording the images and steering angle data - note that we will not use python2 in the next section to access the cameras. TensorFlow is only available in the car's python3 environment, and ROS is only available in our python2 environment. For recording, we do not need access to TensorFlow, only OpenCV. For autonomous driving with both TensorFlow and ROS, we will see how to work around this inconvenience in the next section via zmq . When you are done collecting data, press ctrl-c to terminate collection: Note After you are done with data collection, you might see an error like TypeError: img data type = 17 is not supported printed to the terminal when you ctrl-c - this is harmless and you can safely ignore this error, which happens because the last jpg was not saved to disk due to the ctrl-c interrupt. The rest of the jpgs should be in the saved folder along with a CSV file of jpg filenames, steering angles, and velocities. Note that before recording any data you will need to change the Video Device IDs to the appropriate values, depending on which order the webcams were plugged in and registered by Linux. Set the appropriate values for your car in camera_RACECAR.py class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 The following script will display the currently assigned video device IDs on top of the camera feeds, to help verify the IDs are in the correct order: $ python3 video_id_RACECAR.py python2 should also work above. Sidenote: you can also set udev rules to \"freeze\" these values for your car, if you frequently find the IDs changing after power-cycling the RACECAR. After you have collected your training data, transfer the data using scp or a flashdrive to your laptop and train your model using the provided jupyter notebook . Reminder You should not train a model on the RACECAR","title":"Part 4: RACECAR data collection and training"},{"location":"imitation_learning/#part-5-running-inference-on-racecar","text":"To execute a trained model, you will need to run the following scripts: https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/infer_RACECAR.py https://github.com/mit-alphapilot/imitation_learning_lab/blob/master/drive_RACECAR.py Note We use zmq to send steering angle messages from our Python3 script running inference with TensorFlow, over to a Python2-ROS script that commands the car to drive. You will first need to copy your saved model weights to the RACECAR (e.g., using SCP). You will specify the model location using this command-line argument . Next, if it has changed (due to a reboot or unplugging the cameras), remember to modify the video ID to the center camera here , or verify the current ID is correct using video_id_RACECAR.py : class Video: # dev/video* LEFT = 1 CENTER = 2 RIGHT = 0 Note We only need the center camera during inference. Then, in one terminal, run: $ python3 infer_RACECAR.py --model path_to_model.h5 Ensure you are using python3 above. In another terminal, use python2 and run: $ python2 drive_RACECAR.py Optional exercise This script also includes a mean filter. You can remove this, extend or shorten the length of the mean filter, change it to a median filter, etc, to experiment with inference behavior while driving. visualize_drive.ipynb can be used to overlay steering angle predictions on top of saved runs (see infer_RACECAR.py for a flag that saves images during inference):","title":"Part 5: Running inference on RACECAR"},{"location":"imitation_learning/#tips-and-suggestions","text":"If you are having diffuclty training a working model, here are some suggestions: Visualize your model's predictions by saving the center camera images when you are testing a model (you can use the SAVE_RUN flag) and incorporate some of the code snippets from visualize_drive.ipynb . Are the model outputs noisy (i.e., are the predicted angles jumping around a lot)? Try using the mean or median filter in infer_RACECAR.py . Are the inference angles wrong? Find some images in your training data that come from a similar point in the track where your inferred angles are wrong - how do the model's predictions look there on the training data? If they are also bad, you can try to collect more data in that spot on the track (for instance, you can reset the car to the starting position of a corner several times and record several turns). In addition to visualizing the model's predictions on that section of the track in your training data, also inspect the images and steering angles in the CSV at that point in the track - maybe the car was not being driven smoothly at that location when collecting training data. The dimensions of the model inputs will have a significant impact on the model's accuracy, speed of evaluation (i.e., can you run inference with the model at 30FPS, 15FPS, etc.), and training time/data required (a larger input creates more parameters which may take more training time). How large of an input will you use? The original resolution of the camera frame? Or will you downscale the images? How much will you crop out of the input images? You can choose vertical or horizontal amounts independently of each other. Or you can try using the full camera frame. The number of distinct, unique visual features at each point in the track (i.e., higher input resolutions and larger crops of each frame will include more details from the walls, ceiling, and floor) used when training the model will impact how well the model can memorize the correct steering output to predict at that point in the track during inference, but this will require more training time and data. Smaller inputs can help make the model more robust and generalizable, and reduce the amount of training time. Make sure you can train a model that works on a smaller environment (e.g., around a couple of tables or around a classroom) before tackling the full Stata basement loop. Remember: the training and validation errors are not a great indicator of how well the model wll drive, compared to testing model variants on the car. They are a better indicator of whether the model is continuing to fit (i.e., \"learn\"). Be wary of overfitting: try multiple saved checkpoints instead of just the last one (a checkpoint is saved every epoch). You can shorten the number of training steps per epoch and increase the number of epochs to have more models to try out. You can also try a larger batch size. Try to plan multiple training experiments ahead of time. Instead of changing one hyperparameter, training a model and testing, and going back to change another hyperparameter, try to train several models with different hyperparameters in one session and copy them all over to your racecar. Moreover, if you are using the SAVE_RUN flag, you can try visualizing predictions from all your model variants on the same saved run - you might find another trained model or saved epoch is doing better than the one you were using for testing. You can try adding more model regularization (e.g., more dropout or batchnorm layers) to avoid overfitting. You might also try to decrease the learning rate (you will need to train for longer) - if the learning rate is too high, the model will coverge too quickly. You might also choose to experiment with different activation types (ReLU, ELU, Tanh, ...) since they have different characteristics with respect to backprop. Try training on only the center camera and see if that model performs better. If it does perform better, you might have a poor value selected for your OFFSET_STEERING_ANGLE - it may either be too small or too large, depending on your camera's positioning. Some manually-driven tests may help you to find a better value (every car handles a bit differently). Try significantly increasing the amount of overlap between the three cameras. Make sure they are all level with each other if you re-position the cameras. You will need to unaffix and retape your cameras down while looking at the camera stream (using x-forwarding over ssh). Though this might reduce the chances of your car recovering if it makes a bad turn, it will effectively triple the amount of data you are collecting along the nominal path. If you do this, remember to reduce your OFFSET_STEERING_ANGLE value. With x-forwarding to view the live camera stream, or when copying training data from the RACECAR to your laptop or a server, connecting to the car's router via Ethernet will probably be faster than connecting over WiFi. If you do need to stream or copy data over WiFi, try to use the 5GHz network which will probably be faster. Read Nvidia's PilotNet paper and papers which have cited it (e.g., via Google Scholar) for many more ideas (often in methodology sections, and future work discussions). One powerful regularization/generalization technique for training neural networks is multi-task learning . A crude version of this approach might be to add a second output to the network which predicts velocities (also present in your CSV). It is optional whether you choose to command the model's predicted velocities or continue to command a fixed speed (the current behavior of drive_RACECAR.py ) when running inference - the benefits to model regularization may still be gained from this change. However, this will likely require more training data.","title":"Tips and Suggestions"},{"location":"reinforcement_learning/","text":"Reinforcement Learning Lab Introduction Objective: This lab exercise introduces deep reinforcement learning (Deep RL) for autonomous driving in simulation, using only a camera for sensing. Reinforcement learning is distinct from imitation learning: here, the robot learns to explore the environment on its own, with practically no prior information about the world or itself. Through exploration and reinforcement of behaviors which net reward, rather than human-provided examples of behavior to imitate, a robot has the potential to learn novel, optimal techniques which exceed the abilities of humans. Atari games, Go, and StarCraft are a few well-known settings in which Deep RL algorithms have approached or surpassed human expertise. This lab relies on providing the robot with a simulation environment to use as a sandbox for exploration. In particular, we will use a Unity-based simulation environment originally developed by Tawn Kramer for the DonkeyCar RC platform . This lab exercise relies on a Deep RL demonstration by Antonin Raffin , which uses Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) to quickly train the simulated DonkeyCar to drive on a randomly generated track (and also leverages a variational autoencoder based on code from hardmaru ). This demonstration is itself a fork of an earlier repository by Roma Sokolkov , which leveraged another Deep RL algorithm Deep Deterministic Policy Gradient (DDPG) in the simulator. It is instructive to review Antonin Raffin's blogpost regarding the testing he conducted, for ideas and background, as you work on the lab. For the lab exercise we have forked Antonin Raffin's repository in case there are any lab-specific changes to distribute, but the lab fork is otherwise simply tracking the upstream repo by the original authors (Raffin, Sokolov, Kramer, and other contributors/sources ): Cloning the lab locally: $ git clone https://github.com/mit-alphapilot/learning-to-drive-in-5-minutes A review of Reinforcement Learning If needed, OpenAI's Spinning Up in Deep RL is an excellent way to review in greater depth the concepts discussed during lecture. In particular, the lab is based on topics covered in these sections: Basic concepts in RL The derivation of policy gradients The \"vanilla PG\" algorithm Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC) Gym Interface \"Gym\" interfaces refer to a de facto-standard for reinforcement learning in simulation, popularized by OpenAI's Gym environment . Simulation environments frequently support variants of the following two calls: reset() step() Calls to step() usually take in actions and return the next state, observed reward (if any), and auxillary information such as whether the episode is over (allowing the RL algorithm time to make decisions and optionally reset() the environment for the next episode). For a more in-depth explanation on the concepts in a Gym API, read http://gym.openai.com/docs/ . State-space Dimensionality Reduction A variational autoencoder or VAE is used to reduce the size of the state space that the policy must consider at training and inference time. The state space is represented as a small vector of floating point numbers (e.g., 20-30) taken from the VAE's bottleneck encoding (i.e., the \"latent space\" encoding), instead of the full image. This lecture from Prof. Ali Ghodsi at the University of Waterloo is an excellent, brief, and self-contained introduction to the theory and implementation of VAEs. For additional reading, please consult one or more of these references (or inquire during office hours/over Slack): Ali Ghodsi's lecture: https://www.youtube.com/watch?v=uaaqyVS9-rM https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ https://www.tensorflow.org/alpha/tutorials/generative/cvae https://blog.keras.io/building-autoencoders-in-keras.html Fast-forward Labs blog Part 1 and Part 2 hardmaru 's series of posts on world models: http://blog.otoro.net//2018/06/09/world-models-experiments/ Part 1: Downloading the DonkeyCar simulation environment The following link is a Linux build of the Unity Donkey simulation environment from the original author: Download on Google Drive Note The simulation can also be built from source for other platforms, from the donkey tree of the sdsandbox repo using the Unity development platform. Starting the simulator After unzipping the folder and changing into the directory, launch the simulator with: $ ./build_sdsandbox.x86_64 I suggest the following settings of 640x480 , windowed , and Fantastic rendering quality (but this wll depend on your graphics support): Simulator implementations of OpenAI Gym functions: The following links to the simulator's Gym API implementation are provided as reference for your experimentation (changing the implementation is not necessary however). Note that editing these implementations will not require rebuilidng the simulator, making experimentation easier to conduct. step() is implemented through several callbacks: take_action() calc_reward() - note that this depends implicitly on the cross-track error reported by the simulator on_telemetry() This recieves data from the simulator, including front-bumper images from the simulated DonkeyCar current steering angle and velocity cross-track error (\"cte\") reset() sets all counters to zero is_game_over() is simply a combination of checking for collisions (not present in level 0) or crossing a threshhold of tolerated cross-track error Part 2: Installing Deep RL python dependencies If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml Part 3: Training a policy with a pre-trained VAE First, download the pre-trained VAE from the author's Google Drive folder for Level 0 in the same directory you cloned the repository into. Next, launch the Unity environment (if it is not already running from Part 1 ) To launch a newly initialized training run for the PPO algorithm across 5000 iterations in Level 0 of the simulator, run the following command: $ python train.py --algo ppo -vae vae-level-0-dim-32.pkl -n 5000 Alternatively, to launch a training run for Soft Actor-Critic (SAC): $ python train.py --algo sac -vae vae-level-0-dim-32.pkl -n 5000 For Level 0 , the simulator will select a random track layout, and the Gym API will be used to reset the simulator using this track layout for each episode when training a new policy. Example Track Layout 1 Example Track Layout 2 Question In addition to accumulated reward, what metrics could be used for measuring a policy's performance? Are 5000 steps enough to train a good policy for PPO or SAC? Across different training runs (each starting from a randomly initialized policy), how frequently will PPO or SAC converge on a good policy, versus suffering from performance collapse? Part 4: Experimenting with Deep RL Once you have tried training a policy in the simulation environment, you can experiment with changing the existing algorithms, or try a different Deep RL algorithm altogether, such as TRPO , TD3 , etc. The goal of this section of the lab is to gain some intuition and experience with training the vehicle's policy using deep reinforcement learning, through modifying the existing code, hyperparameters, and algorithms, or by incorporating new algorithms. Your experimentation can target one or more threads of investigation (this is a non-exhaustive list): How can the training performance (accuracy/speed/etc) and learned policy's effectiveness be visualized or quantified? What is the effect of tuning hyperparameters on the convergence time and robustness (or lack thereof) of algorithms like PPO and SAC? What changes to the algorithm can be made to improve convergence behaviors and the robustness of the learned policy? Here are a few examples of possible things to try (again, non-exhaustive): Visualize the policy's training performance (e.g., with tensorboard ) Visualize the value network's training performance (if you are using an actor-critic algorithm like PPO) Alter the hand-crafted reward function by stating a hypothesis, changing the reward calculation, and retraining. See Antonin Raffin's blogpost for his explanation of the current reward function An example hypothesis: perhaps the current implementation of the 'smooth' steering constraint is leading to frequent performance collapse - an alternative implementation may do better. Quantify the variance of a trained policy E.g., what is the distribution of collected reward across multiple trajectories using a trained policy? Can this be used to inform further training of the policy? Change the network to use pixels directly instead of using the VAE encoding. Suggestion: Consider using a CNN instead of a dense network, and explore augmentation/subsampling. Train a policy that can drive on random roads (the simulator is currently set up to use the same road for every episode) Replace the pre-trained VAE with one you trained yourself on collected data in the simulator (this is the first component of Part 5 below) Part 5: Retraining the VAE You can sample from the pre-trained VAE's manifold with the following command: $ python -m vae.enjoy_latent -vae vae-level-0-dim-32.pkl You can move some of the sliders around and \"generate\" new views of the track by running the encoded representation through the deconvolutional portion of the VAE network. See the video below for an example output: To get some hands-on experience with VAEs, collect a new set of images and train a new VAE, instead of using the pre-trained VAE. Optionally consider using a different network architecture (for instance, some recent VAE research has focused on improved disentangling of latent factors, such as this IBM paper among others). You can use the VAE training script and train on new data via: $ python -m vae.train --n-epochs 50 --verbose 0 --z-size 64 -f path-to-record/folder/ where z-size specifies the number of latent factors in the bottleneck. Next, try to train a new VAE network on real-world data, e.g., using the camera images you collected in the Imitation Learning Lab from a classroom or the full Stata basement track. Visualize samples from the manifold of this VAE. Optional Exercise If you are feeling particularly eager and have the time, you could use this VAE to train the RACECAR in a classroom or even in Stata basement! You might want to set up a few extra Traxxas batteries to keep charging and swapping out, because this could take an hour or more of driving on-policy, physically resetting when the car gets near an obstacle or veers off course, retraining, and repeating.","title":"Reinforcement Learning Lab"},{"location":"reinforcement_learning/#reinforcement-learning-lab","text":"","title":"Reinforcement Learning Lab"},{"location":"reinforcement_learning/#introduction","text":"Objective: This lab exercise introduces deep reinforcement learning (Deep RL) for autonomous driving in simulation, using only a camera for sensing. Reinforcement learning is distinct from imitation learning: here, the robot learns to explore the environment on its own, with practically no prior information about the world or itself. Through exploration and reinforcement of behaviors which net reward, rather than human-provided examples of behavior to imitate, a robot has the potential to learn novel, optimal techniques which exceed the abilities of humans. Atari games, Go, and StarCraft are a few well-known settings in which Deep RL algorithms have approached or surpassed human expertise. This lab relies on providing the robot with a simulation environment to use as a sandbox for exploration. In particular, we will use a Unity-based simulation environment originally developed by Tawn Kramer for the DonkeyCar RC platform . This lab exercise relies on a Deep RL demonstration by Antonin Raffin , which uses Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC) to quickly train the simulated DonkeyCar to drive on a randomly generated track (and also leverages a variational autoencoder based on code from hardmaru ). This demonstration is itself a fork of an earlier repository by Roma Sokolkov , which leveraged another Deep RL algorithm Deep Deterministic Policy Gradient (DDPG) in the simulator. It is instructive to review Antonin Raffin's blogpost regarding the testing he conducted, for ideas and background, as you work on the lab. For the lab exercise we have forked Antonin Raffin's repository in case there are any lab-specific changes to distribute, but the lab fork is otherwise simply tracking the upstream repo by the original authors (Raffin, Sokolov, Kramer, and other contributors/sources ):","title":"Introduction"},{"location":"reinforcement_learning/#cloning-the-lab-locally","text":"$ git clone https://github.com/mit-alphapilot/learning-to-drive-in-5-minutes","title":"Cloning the lab locally:"},{"location":"reinforcement_learning/#a-review-of-reinforcement-learning","text":"If needed, OpenAI's Spinning Up in Deep RL is an excellent way to review in greater depth the concepts discussed during lecture. In particular, the lab is based on topics covered in these sections: Basic concepts in RL The derivation of policy gradients The \"vanilla PG\" algorithm Proximal Policy Optimization (PPO) Soft Actor-Critic (SAC)","title":"A review of Reinforcement Learning"},{"location":"reinforcement_learning/#gym-interface","text":"\"Gym\" interfaces refer to a de facto-standard for reinforcement learning in simulation, popularized by OpenAI's Gym environment . Simulation environments frequently support variants of the following two calls: reset() step() Calls to step() usually take in actions and return the next state, observed reward (if any), and auxillary information such as whether the episode is over (allowing the RL algorithm time to make decisions and optionally reset() the environment for the next episode). For a more in-depth explanation on the concepts in a Gym API, read http://gym.openai.com/docs/ .","title":"Gym Interface"},{"location":"reinforcement_learning/#state-space-dimensionality-reduction","text":"A variational autoencoder or VAE is used to reduce the size of the state space that the policy must consider at training and inference time. The state space is represented as a small vector of floating point numbers (e.g., 20-30) taken from the VAE's bottleneck encoding (i.e., the \"latent space\" encoding), instead of the full image. This lecture from Prof. Ali Ghodsi at the University of Waterloo is an excellent, brief, and self-contained introduction to the theory and implementation of VAEs. For additional reading, please consult one or more of these references (or inquire during office hours/over Slack): Ali Ghodsi's lecture: https://www.youtube.com/watch?v=uaaqyVS9-rM https://jaan.io/what-is-variational-autoencoder-vae-tutorial/ https://www.tensorflow.org/alpha/tutorials/generative/cvae https://blog.keras.io/building-autoencoders-in-keras.html Fast-forward Labs blog Part 1 and Part 2 hardmaru 's series of posts on world models: http://blog.otoro.net//2018/06/09/world-models-experiments/","title":"State-space Dimensionality Reduction"},{"location":"reinforcement_learning/#part-1-downloading-the-donkeycar-simulation-environment","text":"The following link is a Linux build of the Unity Donkey simulation environment from the original author: Download on Google Drive Note The simulation can also be built from source for other platforms, from the donkey tree of the sdsandbox repo using the Unity development platform.","title":"Part 1: Downloading the DonkeyCar simulation environment"},{"location":"reinforcement_learning/#starting-the-simulator","text":"After unzipping the folder and changing into the directory, launch the simulator with: $ ./build_sdsandbox.x86_64 I suggest the following settings of 640x480 , windowed , and Fantastic rendering quality (but this wll depend on your graphics support):","title":"Starting the simulator"},{"location":"reinforcement_learning/#simulator-implementations-of-openai-gym-functions","text":"The following links to the simulator's Gym API implementation are provided as reference for your experimentation (changing the implementation is not necessary however). Note that editing these implementations will not require rebuilidng the simulator, making experimentation easier to conduct. step() is implemented through several callbacks: take_action() calc_reward() - note that this depends implicitly on the cross-track error reported by the simulator on_telemetry() This recieves data from the simulator, including front-bumper images from the simulated DonkeyCar current steering angle and velocity cross-track error (\"cte\") reset() sets all counters to zero is_game_over() is simply a combination of checking for collisions (not present in level 0) or crossing a threshhold of tolerated cross-track error","title":"Simulator implementations of OpenAI Gym functions:"},{"location":"reinforcement_learning/#part-2-installing-deep-rl-python-dependencies","text":"If you do not have a GPU on your computer: # Use TensorFlow without a GPU $ conda env create -f environment.yml Otherwise, if you do have a GPU: # Use TensorFlow with a GPU $ conda env create -f environment-gpu.yml","title":"Part 2: Installing Deep RL python dependencies"},{"location":"reinforcement_learning/#part-3-training-a-policy-with-a-pre-trained-vae","text":"First, download the pre-trained VAE from the author's Google Drive folder for Level 0 in the same directory you cloned the repository into. Next, launch the Unity environment (if it is not already running from Part 1 ) To launch a newly initialized training run for the PPO algorithm across 5000 iterations in Level 0 of the simulator, run the following command: $ python train.py --algo ppo -vae vae-level-0-dim-32.pkl -n 5000 Alternatively, to launch a training run for Soft Actor-Critic (SAC): $ python train.py --algo sac -vae vae-level-0-dim-32.pkl -n 5000 For Level 0 , the simulator will select a random track layout, and the Gym API will be used to reset the simulator using this track layout for each episode when training a new policy. Example Track Layout 1 Example Track Layout 2 Question In addition to accumulated reward, what metrics could be used for measuring a policy's performance? Are 5000 steps enough to train a good policy for PPO or SAC? Across different training runs (each starting from a randomly initialized policy), how frequently will PPO or SAC converge on a good policy, versus suffering from performance collapse?","title":"Part 3: Training a policy with a pre-trained VAE"},{"location":"reinforcement_learning/#part-4-experimenting-with-deep-rl","text":"Once you have tried training a policy in the simulation environment, you can experiment with changing the existing algorithms, or try a different Deep RL algorithm altogether, such as TRPO , TD3 , etc. The goal of this section of the lab is to gain some intuition and experience with training the vehicle's policy using deep reinforcement learning, through modifying the existing code, hyperparameters, and algorithms, or by incorporating new algorithms. Your experimentation can target one or more threads of investigation (this is a non-exhaustive list): How can the training performance (accuracy/speed/etc) and learned policy's effectiveness be visualized or quantified? What is the effect of tuning hyperparameters on the convergence time and robustness (or lack thereof) of algorithms like PPO and SAC? What changes to the algorithm can be made to improve convergence behaviors and the robustness of the learned policy? Here are a few examples of possible things to try (again, non-exhaustive): Visualize the policy's training performance (e.g., with tensorboard ) Visualize the value network's training performance (if you are using an actor-critic algorithm like PPO) Alter the hand-crafted reward function by stating a hypothesis, changing the reward calculation, and retraining. See Antonin Raffin's blogpost for his explanation of the current reward function An example hypothesis: perhaps the current implementation of the 'smooth' steering constraint is leading to frequent performance collapse - an alternative implementation may do better. Quantify the variance of a trained policy E.g., what is the distribution of collected reward across multiple trajectories using a trained policy? Can this be used to inform further training of the policy? Change the network to use pixels directly instead of using the VAE encoding. Suggestion: Consider using a CNN instead of a dense network, and explore augmentation/subsampling. Train a policy that can drive on random roads (the simulator is currently set up to use the same road for every episode) Replace the pre-trained VAE with one you trained yourself on collected data in the simulator (this is the first component of Part 5 below)","title":"Part 4: Experimenting with Deep RL"},{"location":"reinforcement_learning/#part-5-retraining-the-vae","text":"You can sample from the pre-trained VAE's manifold with the following command: $ python -m vae.enjoy_latent -vae vae-level-0-dim-32.pkl You can move some of the sliders around and \"generate\" new views of the track by running the encoded representation through the deconvolutional portion of the VAE network. See the video below for an example output: To get some hands-on experience with VAEs, collect a new set of images and train a new VAE, instead of using the pre-trained VAE. Optionally consider using a different network architecture (for instance, some recent VAE research has focused on improved disentangling of latent factors, such as this IBM paper among others). You can use the VAE training script and train on new data via: $ python -m vae.train --n-epochs 50 --verbose 0 --z-size 64 -f path-to-record/folder/ where z-size specifies the number of latent factors in the bottleneck. Next, try to train a new VAE network on real-world data, e.g., using the camera images you collected in the Imitation Learning Lab from a classroom or the full Stata basement track. Visualize samples from the manifold of this VAE. Optional Exercise If you are feeling particularly eager and have the time, you could use this VAE to train the RACECAR in a classroom or even in Stata basement! You might want to set up a few extra Traxxas batteries to keep charging and swapping out, because this could take an hour or more of driving on-policy, physically resetting when the car gets near an obstacle or veers off course, retraining, and repeating.","title":"Part 5: Retraining the VAE"}]}